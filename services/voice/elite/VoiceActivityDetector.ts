/**
 * Advanced Voice Activity Detection (VAD) System
 *
 * World-class VAD implementation using:
 * - WebRTC-based noise estimation
 * - Energy-based detection with adaptive thresholds
 * - Zero-crossing rate analysis
 * - Spectral flux tracking for speech onset detection
 * - Smoothing filters to reduce false triggers
 *
 * Performance targets:
 * - Response time: < 20ms
 * - False positive rate: < 1%
 * - Speech detection accuracy: > 98%
 * - Works in noisy environments up to 60dB SNR
 */

// VAD Configuration
export interface VADConfig {
  // Energy thresholds
  silenceThreshold?: number       // Below this = silence (0-1)
  speechThreshold?: number        // Above this = speech (0-1)

  // Timing parameters (ms)
  speechPadStart?: number         // Padding before speech detection
  speechPadEnd?: number           // Padding after speech ends
  minSpeechDuration?: number      // Minimum duration to count as speech
  maxSilenceDuration?: number     // Max silence before ending speech

  // Adaptive parameters
  adaptiveThreshold?: boolean     // Auto-adjust thresholds
  adaptationRate?: number         // How fast to adapt (0-1)

  // Callbacks
  onSpeechStart?: () => void
  onSpeechEnd?: (duration: number) => void
  onVoiceActivity?: (isActive: boolean, confidence: number) => void
  onAudioLevel?: (level: number, smoothedLevel: number) => void
  onError?: (error: string) => void
}

// VAD State
export interface VADState {
  isActive: boolean
  isSpeaking: boolean
  currentLevel: number
  smoothedLevel: number
  noiseFloor: number
  speechDuration: number
  silenceDuration: number
  confidence: number
}

// Audio analysis metrics
interface AudioMetrics {
  energy: number
  zeroCrossingRate: number
  spectralFlux: number
  spectralCentroid: number
  lowFreqEnergy: number
  highFreqEnergy: number
}

/**
 * Voice Activity Detector Class
 */
export class VoiceActivityDetector {
  private audioContext: AudioContext | null = null
  private analyser: AnalyserNode | null = null
  private source: MediaStreamAudioSourceNode | null = null
  private mediaStream: MediaStream | null = null
  private animationFrame: number | null = null

  // Configuration
  private config: Required<VADConfig>

  // State
  private state: VADState = {
    isActive: false,
    isSpeaking: false,
    currentLevel: 0,
    smoothedLevel: 0,
    noiseFloor: 0.02,
    speechDuration: 0,
    silenceDuration: 0,
    confidence: 0
  }

  // Buffers for analysis
  private timeData: Float32Array = new Float32Array(0)
  private freqData: Float32Array = new Float32Array(0)
  private previousFreqData: Float32Array = new Float32Array(0)
  private levelHistory: number[] = []

  // Timing
  private lastProcessTime = 0
  private speechStartTime = 0

  // Smoothing
  private readonly smoothingFactor = 0.3
  private readonly historySize = 30

  // Adaptive threshold
  private adaptiveNoiseFloor = 0.02
  private adaptiveSpeechLevel = 0.1

  constructor(config: VADConfig = {}) {
    this.config = {
      silenceThreshold: config.silenceThreshold ?? 0.015,
      speechThreshold: config.speechThreshold ?? 0.04,
      speechPadStart: config.speechPadStart ?? 100,
      speechPadEnd: config.speechPadEnd ?? 300,
      minSpeechDuration: config.minSpeechDuration ?? 200,
      maxSilenceDuration: config.maxSilenceDuration ?? 1500,
      adaptiveThreshold: config.adaptiveThreshold ?? true,
      adaptationRate: config.adaptationRate ?? 0.05,
      onSpeechStart: config.onSpeechStart ?? (() => {}),
      onSpeechEnd: config.onSpeechEnd ?? (() => {}),
      onVoiceActivity: config.onVoiceActivity ?? (() => {}),
      onAudioLevel: config.onAudioLevel ?? (() => {}),
      onError: config.onError ?? (() => {})
    }
  }

  /**
   * Start VAD processing
   */
  async start(): Promise<void> {
    if (this.state.isActive) {
      console.warn('VAD already active')
      return
    }

    try {
      // Request microphone access with echo cancellation
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      // Create audio context
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext
      this.audioContext = new AudioContextClass()

      // Create analyser with appropriate settings
      this.analyser = this.audioContext.createAnalyser()
      this.analyser.fftSize = 2048
      this.analyser.smoothingTimeConstant = 0.5
      this.analyser.minDecibels = -90
      this.analyser.maxDecibels = -10

      // Connect source
      this.source = this.audioContext.createMediaStreamSource(this.mediaStream)
      this.source.connect(this.analyser)

      // Initialize buffers
      this.timeData = new Float32Array(this.analyser.fftSize)
      this.freqData = new Float32Array(this.analyser.frequencyBinCount)
      this.previousFreqData = new Float32Array(this.analyser.frequencyBinCount)

      // Start processing
      this.state.isActive = true
      this.lastProcessTime = performance.now()
      this.processAudio()

      console.log('âœ… Voice Activity Detector started')

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Failed to start VAD'
      this.config.onError(errorMsg)
      throw error
    }
  }

  /**
   * Stop VAD processing
   */
  stop(): void {
    this.state.isActive = false

    // Cancel animation frame
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame)
      this.animationFrame = null
    }

    // Disconnect audio nodes
    if (this.source) {
      this.source.disconnect()
      this.source = null
    }

    if (this.analyser) {
      this.analyser.disconnect()
      this.analyser = null
    }

    // Stop media stream
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop())
      this.mediaStream = null
    }

    // Close audio context
    if (this.audioContext) {
      this.audioContext.close().catch(() => {})
      this.audioContext = null
    }

    // Reset state
    this.state = {
      isActive: false,
      isSpeaking: false,
      currentLevel: 0,
      smoothedLevel: 0,
      noiseFloor: 0.02,
      speechDuration: 0,
      silenceDuration: 0,
      confidence: 0
    }

    this.levelHistory = []

    console.log('VAD stopped')
  }

  /**
   * Process audio frame
   */
  private processAudio(): void {
    if (!this.state.isActive || !this.analyser) return

    // Get audio data
    this.analyser.getFloatTimeDomainData(this.timeData)

    // Store previous frequency data for spectral flux
    this.previousFreqData.set(this.freqData)
    this.analyser.getFloatFrequencyData(this.freqData)

    // Calculate timing
    const now = performance.now()
    const deltaMs = now - this.lastProcessTime
    this.lastProcessTime = now

    // Analyze audio
    const metrics = this.analyzeAudio()

    // Update state
    this.updateState(metrics, deltaMs)

    // Report levels
    this.config.onAudioLevel(this.state.currentLevel, this.state.smoothedLevel)
    this.config.onVoiceActivity(this.state.isSpeaking, this.state.confidence)

    // Schedule next frame
    this.animationFrame = requestAnimationFrame(() => this.processAudio())
  }

  /**
   * Analyze audio metrics
   */
  private analyzeAudio(): AudioMetrics {
    const energy = this.calculateEnergy()
    const zeroCrossingRate = this.calculateZeroCrossingRate()
    const spectralFlux = this.calculateSpectralFlux()
    const spectralCentroid = this.calculateSpectralCentroid()
    const { lowFreqEnergy, highFreqEnergy } = this.calculateBandEnergies()

    return {
      energy,
      zeroCrossingRate,
      spectralFlux,
      spectralCentroid,
      lowFreqEnergy,
      highFreqEnergy
    }
  }

  /**
   * Calculate RMS energy
   */
  private calculateEnergy(): number {
    let sum = 0
    for (let i = 0; i < this.timeData.length; i++) {
      sum += this.timeData[i] * this.timeData[i]
    }
    return Math.sqrt(sum / this.timeData.length)
  }

  /**
   * Calculate zero-crossing rate (indicates speech vs noise)
   */
  private calculateZeroCrossingRate(): number {
    let crossings = 0
    for (let i = 1; i < this.timeData.length; i++) {
      if ((this.timeData[i] >= 0) !== (this.timeData[i - 1] >= 0)) {
        crossings++
      }
    }
    return crossings / this.timeData.length
  }

  /**
   * Calculate spectral flux (indicates speech onset)
   */
  private calculateSpectralFlux(): number {
    let flux = 0
    for (let i = 0; i < this.freqData.length; i++) {
      const diff = this.freqData[i] - this.previousFreqData[i]
      flux += Math.max(0, diff) // Only positive changes (onset)
    }
    return flux / this.freqData.length
  }

  /**
   * Calculate spectral centroid (brightness indicator)
   */
  private calculateSpectralCentroid(): number {
    let weightedSum = 0
    let sum = 0

    for (let i = 0; i < this.freqData.length; i++) {
      const magnitude = Math.pow(10, this.freqData[i] / 20) // Convert dB to linear
      weightedSum += i * magnitude
      sum += magnitude
    }

    return sum > 0 ? weightedSum / sum : 0
  }

  /**
   * Calculate low and high frequency band energies
   */
  private calculateBandEnergies(): { lowFreqEnergy: number; highFreqEnergy: number } {
    const nyquist = this.audioContext?.sampleRate ? this.audioContext.sampleRate / 2 : 22050
    const binWidth = nyquist / this.freqData.length

    // Speech typically 300Hz - 3400Hz
    const lowCutoff = Math.floor(300 / binWidth)
    const highCutoff = Math.floor(3400 / binWidth)

    let lowEnergy = 0
    let highEnergy = 0

    for (let i = 0; i < this.freqData.length; i++) {
      const magnitude = Math.pow(10, this.freqData[i] / 20)
      if (i < lowCutoff) {
        lowEnergy += magnitude
      } else if (i <= highCutoff) {
        highEnergy += magnitude
      }
    }

    return {
      lowFreqEnergy: lowEnergy / lowCutoff,
      highFreqEnergy: highEnergy / (highCutoff - lowCutoff)
    }
  }

  /**
   * Update VAD state based on metrics
   */
  private updateState(metrics: AudioMetrics, deltaMs: number): void {
    // Update current level
    this.state.currentLevel = metrics.energy

    // Smooth the level
    this.state.smoothedLevel = this.smoothingFactor * metrics.energy +
      (1 - this.smoothingFactor) * this.state.smoothedLevel

    // Update level history
    this.levelHistory.push(metrics.energy)
    if (this.levelHistory.length > this.historySize) {
      this.levelHistory.shift()
    }

    // Adaptive threshold update
    if (this.config.adaptiveThreshold) {
      this.updateAdaptiveThresholds(metrics)
    }

    // Calculate speech probability
    const speechProbability = this.calculateSpeechProbability(metrics)
    this.state.confidence = speechProbability

    // Determine if currently speaking
    const effectiveSpeechThreshold = this.config.adaptiveThreshold
      ? this.adaptiveSpeechLevel
      : this.config.speechThreshold

    const effectiveSilenceThreshold = this.config.adaptiveThreshold
      ? this.adaptiveNoiseFloor * 1.5
      : this.config.silenceThreshold

    const isSpeechDetected = speechProbability > 0.6 &&
      metrics.energy > effectiveSpeechThreshold

    const isSilence = metrics.energy < effectiveSilenceThreshold

    // State machine for speech detection
    if (this.state.isSpeaking) {
      // Currently in speech state
      if (isSilence) {
        this.state.silenceDuration += deltaMs
        if (this.state.silenceDuration >= this.config.maxSilenceDuration) {
          // End of speech
          const speechDuration = performance.now() - this.speechStartTime
          if (speechDuration >= this.config.minSpeechDuration) {
            this.config.onSpeechEnd(speechDuration)
          }
          this.state.isSpeaking = false
          this.state.speechDuration = 0
          this.state.silenceDuration = 0
        }
      } else {
        // Still speaking
        this.state.speechDuration += deltaMs
        this.state.silenceDuration = 0
      }
    } else {
      // Not currently speaking
      if (isSpeechDetected) {
        this.state.speechDuration += deltaMs
        if (this.state.speechDuration >= this.config.speechPadStart) {
          // Start of speech
          this.state.isSpeaking = true
          this.state.silenceDuration = 0
          this.speechStartTime = performance.now()
          this.config.onSpeechStart()
        }
      } else {
        this.state.speechDuration = 0
      }
    }
  }

  /**
   * Update adaptive thresholds
   */
  private updateAdaptiveThresholds(metrics: AudioMetrics): void {
    const rate = this.config.adaptationRate

    // Update noise floor (during silence)
    if (metrics.energy < this.adaptiveNoiseFloor * 2) {
      this.adaptiveNoiseFloor = (1 - rate) * this.adaptiveNoiseFloor + rate * metrics.energy
      this.state.noiseFloor = this.adaptiveNoiseFloor
    }

    // Update speech level (during speech)
    if (metrics.energy > this.adaptiveSpeechLevel * 0.8 && this.state.isSpeaking) {
      this.adaptiveSpeechLevel = (1 - rate * 0.5) * this.adaptiveSpeechLevel + (rate * 0.5) * metrics.energy
    }

    // Keep reasonable bounds
    this.adaptiveNoiseFloor = Math.max(0.005, Math.min(0.1, this.adaptiveNoiseFloor))
    this.adaptiveSpeechLevel = Math.max(this.adaptiveNoiseFloor * 2, Math.min(0.5, this.adaptiveSpeechLevel))
  }

  /**
   * Calculate speech probability using multiple features
   */
  private calculateSpeechProbability(metrics: AudioMetrics): number {
    // Feature weights
    const weights = {
      energy: 0.35,
      zcr: 0.15,
      spectralFlux: 0.2,
      bandRatio: 0.3
    }

    // Energy score (normalized)
    const noiseFloor = this.config.adaptiveThreshold ? this.adaptiveNoiseFloor : this.config.silenceThreshold
    const energyScore = Math.min(1, Math.max(0, (metrics.energy - noiseFloor) / 0.1))

    // ZCR score (speech typically has ZCR between 0.1 and 0.4)
    const optimalZCR = 0.25
    const zcrScore = 1 - Math.min(1, Math.abs(metrics.zeroCrossingRate - optimalZCR) / 0.25)

    // Spectral flux score (speech has higher spectral flux than noise)
    const fluxScore = Math.min(1, metrics.spectralFlux / 5)

    // Band ratio score (speech has more energy in 300-3400Hz band)
    const totalBandEnergy = metrics.lowFreqEnergy + metrics.highFreqEnergy
    const bandRatioScore = totalBandEnergy > 0
      ? Math.min(1, metrics.highFreqEnergy / totalBandEnergy * 2)
      : 0

    // Weighted combination
    const probability =
      weights.energy * energyScore +
      weights.zcr * zcrScore +
      weights.spectralFlux * fluxScore +
      weights.bandRatio * bandRatioScore

    return Math.min(1, Math.max(0, probability))
  }

  /**
   * Get current state
   */
  getState(): VADState {
    return { ...this.state }
  }

  /**
   * Update configuration
   */
  updateConfig(config: Partial<VADConfig>): void {
    this.config = { ...this.config, ...config } as Required<VADConfig>
  }

  /**
   * Reset adaptive thresholds
   */
  resetAdaptiveThresholds(): void {
    this.adaptiveNoiseFloor = 0.02
    this.adaptiveSpeechLevel = 0.1
    this.state.noiseFloor = 0.02
    this.levelHistory = []
  }

  /**
   * Check if VAD is supported
   */
  static isSupported(): boolean {
    return !!(
      typeof window !== 'undefined' &&
      typeof navigator?.mediaDevices?.getUserMedia === 'function' &&
      (window.AudioContext || (window as any).webkitAudioContext)
    )
  }

  /**
   * Cleanup resources
   */
  destroy(): void {
    this.stop()
  }
}

// Export factory function
export function createVoiceActivityDetector(config?: VADConfig): VoiceActivityDetector {
  return new VoiceActivityDetector(config)
}
